package client

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"time"
)

// OllamaClient ã¯Ollama APIã¨ã®é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
type OllamaClient interface {
	// Generate ã¯LLMã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã„ã¾ã™
	Generate(ctx context.Context, model string, prompt string) (string, error)
	WarmUp(ctx context.Context, model string) error
}

// ollamaClient ã¯OllamaClientã®å®Ÿè£…
type ollamaClient struct {
	baseURL    string
	httpClient *http.Client
}

// NewOllamaClient ã¯æ–°ã—ã„Ollama Clientã‚’ä½œæˆã—ã¾ã™
func NewOllamaClient(baseURL string) OllamaClient {
	return &ollamaClient{
		baseURL: baseURL,
		httpClient: &http.Client{
			Timeout: 3 * time.Minute, // LLMç”Ÿæˆã¯æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é•·ã‚ã«è¨­å®š
		},
	}
}

// OllamaGenerateRequest ã¯Ollama APIã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼
type OllamaGenerateRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
	Stream bool   `json:"stream"`
}

// OllamaGenerateResponse ã¯Ollama APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ï¼ˆNon-streamingï¼‰
type OllamaGenerateResponse struct {
	Model     string `json:"model"`
	CreatedAt string `json:"created_at"`
	Response  string `json:"response"`
	Done      bool   `json:"done"`
}

// WarmUp ã¯ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«Ollamaã‚’æº–å‚™çŠ¶æ…‹ã«ã—ã¾ã™
func (c *ollamaClient) WarmUp(ctx context.Context, model string) error {
	log.Println("ğŸ”¥ [Ollama] Starting warmup...")

	// Step 1: Health check
	if err := c.waitForReady(ctx, 60*time.Second); err != nil {
		return fmt.Errorf("ollama not ready: %w", err)
	}

	// Step 2: ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
	exists, err := c.checkModelExists(ctx, model)
	if err != nil {
		return fmt.Errorf("failed to check model: %w", err)
	}

	if !exists {
		log.Printf("âš ï¸  [Ollama] Model '%s' not found", model)

		// ç’°å¢ƒå¤‰æ•°ã§è‡ªå‹•ãƒ—ãƒ«ã‚’åˆ¶å¾¡
		if os.Getenv("OLLAMA_AUTO_PULL") != "true" {
			return fmt.Errorf("model '%s' not found. Run 'ollama pull %s' first, or set OLLAMA_AUTO_PULL=true", model, model)
		}

		log.Printf("ğŸ“¥ [Ollama] Auto-pulling model '%s'...", model)
		if err := c.pullModel(ctx, model); err != nil {
			return fmt.Errorf("failed to pull model: %w", err)
		}
	} else {
		log.Printf("âœ… [Ollama] Model '%s' already exists", model)
	}

	// Step 3: ãƒ€ãƒŸãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
	log.Printf("ğŸ”„ [Ollama] Loading model into memory: %s", model)

	_, err = c.Generate(ctx, model, "warmup")
	if err != nil {
		return fmt.Errorf("failed to load model: %w", err)
	}

	log.Printf("âœ… [Ollama] Model loaded: %s", model)
	log.Println("âœ… [Ollama] Warmup complete!")
	return nil
}

// checkModelExists ã¯ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
func (c *ollamaClient) checkModelExists(ctx context.Context, model string) (bool, error) {
	url := fmt.Sprintf("%s/api/tags", c.baseURL)
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return false, err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return false, err
	}
	defer resp.Body.Close()

	var result struct {
		Models []struct {
			Name string `json:"name"`
		} `json:"models"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return false, err
	}

	for _, m := range result.Models {
		if m.Name == model {
			return true, nil
		}
	}

	return false, nil
}

// pullModel ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
// pullModel ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
func (c *ollamaClient) pullModel(ctx context.Context, model string) error {
	log.Printf("ğŸ“¥ [Ollama] Pulling model '%s' (this may take 2-5 minutes)...", model)

	url := fmt.Sprintf("%s/api/pull", c.baseURL)

	reqBody := map[string]interface{}{
		"name": model,
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return err
	}

	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return err
	}

	req.Header.Set("Content-Type", "application/json")

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("pull failed with status %d: %s", resp.StatusCode, string(body))
	}

	// ãƒ—ãƒ«é€²æ—ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§èª­ã‚€ï¼ˆæ”¹å–„ç‰ˆï¼‰
	decoder := json.NewDecoder(resp.Body)
	lastStatus := ""

	for {
		var progress struct {
			Status    string `json:"status"`
			Completed int64  `json:"completed,omitempty"`
			Total     int64  `json:"total,omitempty"`
		}

		if err := decoder.Decode(&progress); err != nil {
			if err == io.EOF {
				break
			}
			return fmt.Errorf("failed to read pull progress: %w", err)
		}

		// åŒã˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é€£ç¶šã‚’é¿ã‘ã‚‹
		currentStatus := progress.Status
		if progress.Total > 0 {
			percentage := float64(progress.Completed) / float64(progress.Total) * 100
			currentStatus = fmt.Sprintf("%s (%.1f%%)", progress.Status, percentage)
		}

		if currentStatus != lastStatus {
			log.Printf("ğŸ“¥ [Ollama] %s", currentStatus)
			lastStatus = currentStatus
		}
	}

	log.Printf("âœ… [Ollama] Model '%s' pulled successfully", model)
	return nil
}

// waitForReady ã¯OllamaãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™
func (c *ollamaClient) waitForReady(ctx context.Context, timeout time.Duration) error {
	deadline := time.Now().Add(timeout)
	retryInterval := 2 * time.Second

	for time.Now().Before(deadline) {
		// ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		// Health check ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å©ã
		url := fmt.Sprintf("%s/api/tags", c.baseURL)
		req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
		if err != nil {
			return fmt.Errorf("failed to create health check request: %w", err)
		}

		resp, err := c.httpClient.Do(req)
		if err == nil && resp.StatusCode == http.StatusOK {
			resp.Body.Close()
			log.Println("âœ… [Ollama] Service is ready")
			return nil
		}

		if resp != nil {
			resp.Body.Close()
		}

		log.Printf("â³ [Ollama] Waiting for service... (retrying in %v)", retryInterval)

		// ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒ³ã‚»ãƒ«ã«ã‚‚å¯¾å¿œï¼‰
		select {
		case <-time.After(retryInterval):
			// æ¬¡ã®ãƒªãƒˆãƒ©ã‚¤ã¸
		case <-ctx.Done():
			return ctx.Err()
		}
	}

	return fmt.Errorf("timeout waiting for ollama (waited %v)", timeout)
}

// Generate ã¯æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™
func (c *ollamaClient) Generate(ctx context.Context, model string, prompt string) (string, error) {
	// Step 1: ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ä½œæˆ
	reqBody := OllamaGenerateRequest{
		Model:  model,
		Prompt: prompt,
		Stream: false, // Non-streamingãƒ¢ãƒ¼ãƒ‰
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	// Step 2: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
	url := fmt.Sprintf("%s/api/generate", c.baseURL)
	req, err := http.NewRequestWithContext(
		ctx,
		"POST",
		url,
		bytes.NewBuffer(jsonData),
	)
	if err != nil {
		return "", fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")

	// Step 3: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", fmt.Errorf("failed to send request to Ollama: %w", err)
	}
	defer resp.Body.Close()

	// Step 4: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("ollama API returned status %d: %s", resp.StatusCode, string(body))
	}

	// Step 5: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ã‚³ãƒ¼ãƒ‰
	var response OllamaGenerateResponse
	if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
		return "", fmt.Errorf("failed to decode response: %w", err)
	}

	// Step 6: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
	return response.Response, nil
}
